{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ITS_LIVE** Global Glacier Velocity Exploration and Analysis. <a class=\"anchor\" id=\"chapter_1\"/>\n",
    "<img title=\"ITS_LIVE\" src=\"img/header.png\" width=\"50%\"/>\n",
    "\n",
    "Luis Lopez[<sup>1</sup>](#fn1), Alex Gardner[<sup>2</sup>](#fn2), Mark Fahnestock[<sup>3</sup>](#fn3), Ted Scambos[<sup>4</sup>](#fn4), Maria Liukis[<sup>2</sup>](#fn2), Chad Greene[<sup>2</sup>](#fn2), Yang Lei[<sup>5</sup>](#fn5), Joe Kennedy[<sup>3</sup>](#fn3),  Bruce Wallin[<sup>1</sup>](#fn1)\n",
    " \n",
    "[![Binder](https://binder.pangeo.io/badge_logo.svg)](https://binder.pangeo.io/v2/gh/nasa-jpl/itslive-explorer/earthcube?filepath=LL_01_ITS_LIVE_global_glacier_velocity_exploration_and_analysis.ipynb)\n",
    "\n",
    "<span id=\"fn1\" style=\"font-size: small\">1.National Snow and Ice Data Center</span><BR>\n",
    "<span id=\"fn2\" style=\"font-size: small\">2.NASA Jet Propulsion Laboratory</span><BR>\n",
    "<span id=\"fn3\" style=\"font-size: small\">3.University of Alaska Fairbanks</span><BR>\n",
    "<span id=\"fn4\" style=\"font-size: small\">4.University of Colorado Earth Science and Observation Center</span><BR>\n",
    "<span id=\"fn5\" style=\"font-size: small\">5.California Institute of Technology</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s) <a class=\"anchor\" id=\"section_1_1\"/>\n",
    "\n",
    "- Author1 = {\"name\": \"Luis Lopez\", \"affiliation\": \"National Snow and Ice Data Center\", \"email\": \"luis.lopez@nsidc.org\", \"orcid\": \"\"}\n",
    "- Author2 = {\"name\": \"Alex Gardner\", \"affiliation\": \"NASA Jet Propulsion Laboratory\", \"email\": \"alex.s.gardner@jpl.nasa.gov\", \"orcid\": \"0000-0002-8394-8889\"}\n",
    "- Author3 = {\"name\": \"Mark Fahnestock\", \"affiliation\": \"University of Alaska Fairbanks\", \"email\": \"mfahnestock@alaska.edu\", \"orcid\": \"\"}\n",
    "- Author4 = {\"name\": \"Ted Scambos\", \"affiliation\": \"University of Colorado Earth Science and Observation Center\", \"email\": \"tascambos@colorado.edu\", \"orcid\": \"\"}\n",
    "- Author5 = {\"name\": \"Maria Liukis\", \"affiliation\": \"NASA Jet Propulsion Laboratory\", \"email\": \"maria.liukis@jpl.nasa.gov\", \"orcid\": \"\"}\n",
    "- Author6 = {\"name\": \"Chad Greene\", \"affiliation\": \"NASA Jet Propulsion Laboratory\", \"email\": \"chad.a.greene@jpl.nasa.gov\", \"orcid\": \"\"}\n",
    "- Author7 = {\"name\": \"Yang Lei\", \"affiliation\": \"California Institute of Technology\", \"email\": \"ylei@caltech.edu\", \"orcid\": \"\"}\n",
    "- Author7 = {\"name\": \"Joe Kennedy\", \"affiliation\": \"University of Alaska ASF\", \"email\": \"jhkennedy@alaska.edu\", \"orcid\": \"\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1 ITS_LIVE global glacier velocity exploration and analysis.Â¶](#chapter_1)\n",
    "    * [1.1 Author(s)](#section_1_1)\n",
    "    * [1.2 Purpose](#section_1_2)\n",
    "    * [1.3 Technical contributions](#section_1_3)\n",
    "    * [1.4 Methodology](#section_1_4)\n",
    "    * [1.5 Results](#section_1_5)\n",
    "    * [1.6 Funding](#section_1_6)\n",
    "    * [1.7 Keywords](#section_1_7)\n",
    "    * [1.8 Citation](#section_1_8)\n",
    "    * [1.9 Work In Progress - improvements](#section_1_9)\n",
    "    * [1.10 Suggested next steps](#section_1_10)\n",
    "    * [1.11 Acknowledgements](#section_1_11)\n",
    "* [2 Setup](#chapter_2)\n",
    "    * [2.1 Library import](#section_2_1)\n",
    "    * [2.2  Local library import](#section_2_2)\n",
    "* [3 Parameter definitions](#chapter_3)\n",
    "* [4 Data import](#chapter_4)\n",
    "* [5 Data processing and analysis](#chapter_5)\n",
    "    * [5.1 Building a data cube](#section_5_1)\n",
    "    * [5.2 Plotting a time series with xarray](#section_5_2)\n",
    "* [6 References](#chapter_6)\n",
    "\n",
    "## Purpose <a class=\"anchor\" id=\"section_1_2\"/>\n",
    "\n",
    "The itslive-explorer notebook allows users to explore and visualize glacier surface velocity at global scale using data produced by NASA's JPL autonomous Repeat Image Feature Tracking algorithm (Gardner et al., 2018).\n",
    "\n",
    "Because of its high temporal and spatial resolution, ITS_LIVE data amounts to 8+ million NetCDF files (and growing) stored in AWS S3. For the users' convenience, exploration and filtering of this data can be done using an ipyleaflet-based widget that laverages the [ITS_LIVE search API](https://nsidc.org/apps/itslive-search/docs). After relevant data granules for an area (for example a glacier of interest) are downloaded, the notebook provides users with an xarray-powered method to build a data cube ready for time series analysis and from which valuable scientific insights could be gathered.\n",
    "\n",
    "## Technical contributions <a class=\"anchor\" id=\"section_1_3\"/>\n",
    "\n",
    "* **Demonstration of time series analysis using the ITS_LIVE velocity pairs dataset**\n",
    "  * The main contribution of the notebook is to provide users with a transparent way to access and process glacier surface velocity data. The notebook uses the ITS_LIVE search API to retrieve a list of granules of interest stored on AWS S3. This can be done using the client library or using an ipyleaflet-based widget. The core example of this notebook creates a time series that enables scientists to work on the science and worry less about the code.\n",
    "* **Development of underlying search API that is exposed in the notebook**\n",
    "  * The [ITS_LIVE search API](https://nsidc.org/apps/itslive-search/docs) is a [FastAPI](https://fastapi.tiangolo.com/) application that ingests geojson metadata produced by the AutoRIFT processing pipeline and indexes them using PostGIS.\n",
    "* **Contributing back to the open source community**\n",
    "  * Since a considerable number of glaciers are in polar latitudes, visualization of such glacier boundaries get distorted and visual overlaps make it difficult to work with them. ITS_LIVE wanted to give users who are not familiar with APIs and Python a way to get the information to their machine, independently of whether their machine is a laptop or a VM in the cloud. For this reason we **implemented custom map projections** for the **[ipyleaflet](https://github.com/jupyter-widgets/ipyleaflet)** map widget. This way a user can search and download data without the need to code a single line of Python, as some scientist have their own way of processing data (e.g. matlab) \n",
    "\n",
    "## Methodology <a class=\"anchor\" id=\"section_1_4\"/>\n",
    "\n",
    "### Data Processing \n",
    "\n",
    "Since its conception ITS_LIVE aims to use a cloud native approach to generate and analyze data. Leveraging the fact that the input sources are stored in AWS S3 all the processing occurs on AWS infrastructure using the [hyp3]((https://hyp3.asf.alaska.edu/)) pipeline developed by the **Alaska Satellite Facility**. A dockerized implementation of AutoRIFT is applied to n number of input files in parallel generating output files which are stored back in S3 along with geojson metadata for each granule.\n",
    "\n",
    "<img title=\"ITS_LIVE hyp3 pipeline\" src=\"img/processing-pipeline.png\" width=\"50%\"/> <a class=\"anchor\" id=\"figure_2\"/>\n",
    "\n",
    " [ITS_LIVE's AutoRIFT algorithm](https://github.com/leiyangleon/autoRIFT) can be applied to optical and radar input files. Use cases include the measurement of surface displacements occurring between two repeat satellite images as a result of glacier flow, large earthquake displacements, and land slides. Currently we use Landsat optical and Sentinel radar imagery as input sources for glacier velocity extraction.\n",
    "\n",
    "Image pairs collected from the same satellite position (\"same-path-row\") are searched if they have a time separation of fewer than 546 days. This approach was used for all satellites in the Landsat series (L4 to L8). To increase data density prior to the launch of Landsat 8, images acquired from differing satellite positions, generally in adjacent or near-adjacent orbit swaths (\"cross-path-row\"), are also processed if they have a time separation between 10 and 96 days and an acquisition date prior to 14 June 2013(beginning of regular Landsat 8 data). Feature tracking of cross-path-row image pairs produces velocity fields with a lower signal-to-noise ratio due to residual parallax from imperfect terrain correction. Same-path-row and cross-path-row preprocessed pairs of images are searched for matching features by finding local normalized cross-correlation (NCC) maxima at sub-pixel resolution by oversampling the correlation surface by a factor of 16 using a Gaussian kernel. A sparse grid pixel-integer NCC search (1/16 of the density of full search grid) is used to determine areas of coherent correlation between image pairs. For more information, see the Normalized Displacement Coherence (NDC) filter described in Gardner et al. (2018)\n",
    "\n",
    "Fig 1 shows  vertical and horizontal pixel displacements being correlated to create a final velocity field normalized to meters per year.\n",
    "\n",
    "<img title=\"AutoRIFT\" src=\"https://raw.githubusercontent.com/leiyangleon/autoRIFT/master/figures/regular_grid_optical.png\" width=\"50%\"/> <a class=\"anchor\" id=\"figure_1\"/>Fig 1.\n",
    "\n",
    "### ITS_LIVE velocity pair granules\n",
    "\n",
    "The velocity pair granule is distributed in NetCDF format. \n",
    "\n",
    "* Coverage: All land ice\n",
    "* Date range: 1985-present\n",
    "* Resolution: 240m\n",
    "* Scene-pair separation: 6 to 546 days\n",
    "\n",
    "<img title=\"ITS_LIVE hyp3 pipeline\" src=\"img/velocity-granule.png\" width=\"50%\"/> <a class=\"anchor\" id=\"figure_3\"/>\n",
    "\n",
    "### Granule Search and Analysis\n",
    "It has 2 endpoints to retrieve a list of granules of interest using the OpenAPI specification:\n",
    "\n",
    "  * `/velocities/coverage/`\n",
    "    * gets an aggregate by year (a faceted result) of how many granules will be returned given the user's spatiotemporal parameters \n",
    "  * `/velocities/urls/`\n",
    "    * produces a list of S3 file URLs that match the user's spatiotemporal parameters\n",
    "\n",
    "## Results <a class=\"anchor\" id=\"section_1_5\"/>\n",
    "Describe and comment on the most important results. Include images and URLs as necessary. \n",
    "\n",
    "## Funding <a class=\"anchor\" id=\"section_1_6\"/>\n",
    "\n",
    "- Award1 = {\"agency\": \"NASA\", \"award_code\": \"# Making Earth System Data Records for Use in Research Environments (MEaSUREs) Program\", \"award_URL\": \"https://earthdata.nasa.gov/esds/competitive-programs/measures/its-live\"}\n",
    "\n",
    "## Keywords <a class=\"anchor\" id=\"section_1_7\"/>\n",
    "\n",
    "keywords=[\"glacier\", \"surface\", \"velocity\", \"dataset\", \"nasa\"]\n",
    "\n",
    "## Citation <a class=\"anchor\" id=\"section_1_8\"/>\n",
    "\n",
    "\n",
    "## Work In Progress - improvements <a class=\"anchor\" id=\"section_1_9\"/>\n",
    "\n",
    "The current analysis workflow still relies in the dated download and analyze paradigm but the idea is to use Pangeo's way and thus process the granules on a Dask cluster.  A dedicated Dask cluster for our users is out of scope for ITS_LIVE, however it will be very convenient to adapt the current client library and examples to use one.\n",
    "\n",
    "#### TODOs:\n",
    "- **On demand cube generation**: An intermediate step between the current workflow to analyze the velocity granules locally and in the cloud is to implement the data cube generation on the ITS_LIVE back-end. This way the users will only care about downloading the slice of data they need with the variables they need.\n",
    "- **Velocity basemap**: the current map widget does not have a basemap that reflects the global velocity mosaics. The trick is to adapt gdal2tiles or something like it to process enough regional maps into a partition that is compatible with NASA GIBS grid definitions(the basemaps used in the widget)\n",
    "- **Include elevation change datasets**: Velocity is not the only ingredient scientist need in order to analyze glacier movements, elevation change data is also part of ITS_LIVE but is not yet included on the current notebook. Once we have both datasets, more accurate and interesting analyses will be possible e.g. mass balance trends.\n",
    "- **NASA's Harmony integration**: [NASA's Harmony](https://harmony.earthdata.nasa.gov/) is NASA's next generation data processing tool that will allow scientist to get subsetted and analysis ready data in cloud data format. Having ITS_LIVE as a Harmony compatible data will close the  cloud native circle stated earlier.\n",
    "\n",
    "## Acknowledgements <a class=\"anchor\" id=\"section_1_11\"/>\n",
    "\n",
    "Daniel Tiger and Peppa Pig et al. for entertaining our twin toddlers while finishing this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup <a class=\"anchor\" id=\"chapter_2\"/>\n",
    "\n",
    "\n",
    "\n",
    "## Library import <a class=\"anchor\" id=\"section_2_1\"/>\n",
    "\n",
    "> This notebook was designed to build data cubes on a glacier scale rather than larger areas. This notebook will use Pine Island Glacier for demo purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation and plotting.\n",
    "import xarray as xr\n",
    "\n",
    "# ITS_LIVE Search client which can be used as a widget or just code.\n",
    "from SearchWidget import map\n",
    "# horizontal=render in notebook. vertical = render in sidecar\n",
    "m = map(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definitions  <a class=\"anchor\" id=\"chapter_3\"/>\n",
    "\n",
    "ITS_LIVE data com from multiple scenes and satellites, which means a lot of overlap. In this case all the scenes that match with our spatial criteria will be returned.\n",
    "\n",
    "### Search parameters\n",
    "\n",
    "* **polygon/bbox**: LON-LAT pairs separated by a comma.\n",
    "* **start**: YYYY-mm-dd start date\n",
    "* **end**: YYYY-mm-dd end date\n",
    "* **min_separation**: minimum days of separation between scenes\n",
    "* **max_separation**: maximum days of separation between scenes\n",
    "* **percent_valid_pixels**: % of valid pixel coverage on glaciers\n",
    "* **serialization**: json,html,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying: https://nsidc.org/apps/itslive-search/velocities/urls/?polygon=-101.1555,-74.7387,-99.1172,-75.0879,-99.8797,-75.46,-102.425,-74.925,-101.1555,-74.7387&start=1984-01-01&end=2020-01-01&percent_valid_pixels=40&min_interval=6&max_interval=120\n",
      "Total granules found: 1643\n"
     ]
    }
   ],
   "source": [
    "# Pine Island Glacier\n",
    "params = {\n",
    "    'polygon': '-101.1555,-74.7387,-99.1172,-75.0879,-99.8797,-75.46,-102.425,-74.925,-101.1555,-74.7387',\n",
    "    'start': '1984-01-01',\n",
    "    'end': '2020-01-01',\n",
    "    'percent_valid_pixels': 40,\n",
    "    'min_separation': 6,\n",
    "    'max_separation': 120\n",
    "}\n",
    "\n",
    "granule_urls = m.Search(params)\n",
    "print(f'Total granules found: {len(granule_urls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import  <a class=\"anchor\" id=\"chapter_4\"/>\n",
    "\n",
    "## Data Filtering\n",
    "\n",
    "More than a thousand granules doesn't seem much but it's not trivial if you only want to get a glance of the behavior of a particular glacier over the years. For this reason we can limit the number of granules per year and download only those with a given month as a middate, this is useful if the glacier is affected by seasonal cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997 1\n",
      "2000 1\n",
      "2001 7\n",
      "2002 10\n",
      "2003 5\n",
      "2007 5\n",
      "2008 8\n",
      "2009 7\n",
      "2010 10\n",
      "2011 6\n",
      "2012 10\n",
      "2013 10\n",
      "2014 10\n",
      "2015 10\n",
      "2016 10\n",
      "2017 10\n",
      "2018 10\n",
      "2019 10\n",
      "Total granules after filtering: 140\n"
     ]
    }
   ],
   "source": [
    "# filter_urls requires a list of urls, the result is stored in the m.filtered_urls attribute\n",
    "filtered_granules_by_year = m.filter_urls(granule_urls,\n",
    "                                          max_files_per_year=10,\n",
    "                                          months=['November', 'December', 'January'],\n",
    "                                          by_year=True)\n",
    "\n",
    "# We print the counts per year\n",
    "for k in filtered_granules_by_year:\n",
    "    print(k, len(filtered_granules_by_year[k]))\n",
    "print(f'Total granules after filtering: {len(m.filtered_urls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "\n",
    "We have 2 options to download data, we can download filtered urls (by year or as a whole) or we can donload a whole set of URLs returned in our original search.\n",
    "\n",
    "Single year example:\n",
    "\n",
    "```python\n",
    "files = m.download_velocity_granules(urls=filtered_granules_by_year['2006'],\n",
    "                                     path_prefix='data/pine-glacier-2006',\n",
    "                                     params=params)\n",
    "```\n",
    "\n",
    "The `path_prefix` is the dorectory on which the netcdf files will be downloaded to and `params` is to keep track of which parameters were used to download a particular set of files.\n",
    "\n",
    "We can also download the whole series\n",
    "\n",
    "```python\n",
    "files = m.download_velocity_granules(urls=m.filtered_urls,\n",
    "                                     path_prefix='data/pine-glacier-1996-2019',\n",
    "                                     filtered_urls=params)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_urls = m.filtered_urls # or filtered_granules_by_year\n",
    "project_folder = 'data/pine-1996-2019'\n",
    "\n",
    "# if we are using our parameters (not the widget) we asign our own dict i.e. params=my_params\n",
    "files = m.download_velocity_granules(urls=filtered_urls,\n",
    "                                     path_prefix=project_folder,\n",
    "                                     params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and analysis  <a class=\"anchor\" id=\"chapter_5\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's open a single data granule (included in this notebook)\n",
    "velocity_granule = xr.open_dataset('data/LE07_L1GT_001113_20121118_20161127_01_T2_X_LE07_L1GT_232113_20121104_20161127_01_T2_G0240V01_P059.nc')\n",
    "velocity_granule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xarray has built-in methods to plot our variables \n",
    "velocity_granule.v.plot(x='x', y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a data cube <a class=\"anchor\" id=\"section_5_1\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a time series with xarray <a class=\"anchor\" id=\"section_5_2\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References <a class=\"anchor\" id=\"chapter_6\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
